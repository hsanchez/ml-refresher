\input{preamble}
%% end of preamble

\usepackage{framed}
\usepackage[inline]{enumitem}
\usepackage[backend=bibtex]{biblatex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{gensymb}  % degrees in latex

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\usepackage{multirow}
\usepackage{booktabs}

%% borrowed from https://tex.stackexchange.com/questions/229023/expectation-operator
\usepackage{mathtools}
\providecommand\given{}
\DeclarePairedDelimiterXPP\Aver[1]{\mathbb{E}}{[}{]}{}{
\renewcommand\given{  \nonscript\:
  \delimsize\vert
  \nonscript\:
  \mathopen{}
  \allowbreak}
#1
}


\title[]{\Huge \textbf{\textcolor{black}{A Machine Learning Refresher}}}
\subtitle{\Large Q \& A on the main concepts and terminology}
% \author{\large \textbf{Huascar A. Sanchez}}
\author[HAS]{
% \author[HAS \& DS]{
\parbox[t]{1.5in}{Huascar Sanchez \\\small\texttt{github.com/hsanchez}} %\hspace{.3in}
% \and
% \parbox[t]{1.5in}{Second Author \\\small\texttt{second.author@sri.com}}
}
\institute[]{\large Home Research Lab}

\date{\today}

% Left align title page
\makeatletter
\setbeamertemplate{title page}[default][left,colsep=-4bp,rounded=true,shadow=\beamer@themerounded@shadow]
\makeatother

\begin{document}
%%% TIKZ STUFF
\tikzset{
        every picture/.style={remember picture,baseline},
        every node/.style={anchor=base,align=center,outer sep=1.5pt},
        every path/.style={thick},
        }
\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-a) at (-.3em,.3em) {};%
}
\newcommand\markbottomright[2]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-b) at (0em,0em) {};%
}
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{mybox} =[draw=black, very thick, rectangle, inner sep=10pt, inner ysep=20pt]
\tikzstyle{mybox2} =[draw=black, very thick, rectangle, inner sep=5pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[draw=black,fill=red, text=white]
%%%% END TIKZ STUFF

% Title Slide
\begin{frame}
\maketitle
% \centering
\tiny\hspace{1em}The views expressed do not necessarily reflect the position of my employer.
\end{frame}

% INTRO
\section{General matters}
\begin{frame}[fragile]{\textbf{Q. What is Machine Learning?}}
  \begin{wideitemize}
    \item Machine Learning (or \textbf{ML}) is fitting a function to
    examples\footnote{An example, or a sample, is a data point that belongs
    to some data} and using that function to generalize and make predictions
    about new examples.
    \item Machine Learning, by large, falls into three categories:\vspace{.4em}
    \begin{wideitemize}
      \item[-] Supervised learning
      \item[-] Unsupervised learning
      \item[-] Reinforcement learning
    \end{wideitemize}
    \item And solves two types of problems: Classification and Regression.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Supervised Learning?}}
  \begin{wideitemize}
    \item In \textbf{Supervised Learning} (or \textbf{SL}), you are given a
    bunch of examples and their labels (e.g., A or B) and the goal is to
    classify (or assign), when you are given a new example, to which label we
    assign the new example.
    \item \textcolor{blue}{You could think of these labels the name of the
    \textbf{class or cluster} to which certain portions of the data belong.}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you give an example of Supervised Learning?}}
  \vspace{.4em}
  \begin{wideitemize}
    \item \textcolor{blue}{\textbf{Support vector machines} or \textbf{SVM}}, which
    goal is to construct the optimal separating \textit{hyperplane} between pieces of data; e.g.,
    \begin{wideitemize}
      \item[-] Say a hyperplane between clusters of data represented by labels A and B.
      \item[-] These clusters sit in some high dimensional space, and the idea is to
      construct a plane that maximizes the margins between the plane and the data.
      \item[-] If a new datum sits closer to one area of the data, say A, then
      we assign this new datum to A.
    \end{wideitemize}
    \item (For historical reasons) This algorithm is called \textbf{support
    vector machines} because the vectors that lie on the margin of the plan are
    called the \textit{support} vectors.
  \end{wideitemize}

  \begin{framed}
  This is a method for constructing a device to discriminate. If we're having
  a supervised learning problem then this method gives me an optimal form of
  discrimination.
  \end{framed}

\end{frame}

\note[itemize]{
\item This algorithm has an exponential speed up when you do it quantum mechanically..
}

\begin{frame}[fragile]{\textbf{Q. What is Unsupervised Learning?}}
  \begin{wideitemize}
    \item In \textbf{Unsupervised Learning} (or \textbf{UL}), you are given a
    bunch of data and you are not told it falls naturally into clusters, but
    you are not told what the clusters are.
    \item \textcolor{blue}{The goal is identify the clusters of data, how many clusters there are,
    and then be able to assign new things to these different clusters.}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you give an example of Unsupervised Learning?}}
  \begin{wideitemize}
    \item \textcolor{blue}{Principal Component Analysis (or \textbf{PCA})} is a classical UL algorithm.
    \item In \textbf{PCA}, the way this works, we construct a \textit{covariance matrix},
    and my covariance matrix is just the following object: $C = \sum_{j} \vec{v}_j\vec{v}^{\dagger}_{j}$,
    where $\vec{v}^{T}_{j}$ is the transpose of $\vec{v}_{j}$.
    \begin{wideitemize}
      \item[-] In other words, we construct $C$ from the data by taking these vectors $\vec{v}_j$ and
      multiply them by their transpose $\vec{v}^{\dagger}_j$.
      \item E.g., Financial forecasting: the vectors could be, for example, can
      represent the changes in stock prices over a 24 hr period, and the
      covariance matrix $C$ would give the correlations (or covariances in the
      data) between the prices of the different stocks in different times within
      the 24 hrs period.
    \end{wideitemize}
    \item In \textbf{PCA}, you diagonalize $C$ and say $C = \sum_{k} P_k
    \vec{\omega}_{k}\vec{\omega}^{\dagger}_{k}$, $P_k$ is piece of the data with size $k$,
    and $\vec{\omega}_{k}$ are the set of vectors you need to find.
    \item If and only if a small set of $P_k >> 0$, then $C$ is effectively low-rank,
    and the corresponding $\vec{\omega}_{k}$ are the principal components.
  \end{wideitemize}

\end{frame}

\note[itemize]{
\item In this stocks example, for instance, if it turns out that all the
  motions of stocks in the stocks market are highly correlated with each other AND
  there is only a few forms of correlations, then this matrix $C$ will be
  effectively low-rank and will only have a few principle components.
\item You see, this PCA is a way of compressing the data. That is, if there
  is only a few principal components then this means that all my vectors can
  be written as the sum of a few $w$s.
\item In general terms, this PCA process is about finding the underlying
  patterns of the data and also gives you a method for data compression.
\item PCA is all about diagonalizing the covariance matrix.
\item PCA is an exercise in linear algebra on very high dimensional vector spaces.
}

\begin{frame}[fragile]{\textbf{Q. What is Reinforcement Learning?}}
  \begin{wideitemize}
    \item Reinforcement learning is one of three basic machine learning paradigms, alongside
    supervised learning and unsupervised learning.
    \item In reinforcement learning an algorithm learns to do something by being
    rewarded for successful behavior and/or being punished for unsuccessful behavior.
  \end{wideitemize}
\end{frame}

\note[itemize]{
\item In a supervised learning model, the algorithm learns on a labeled
dataset, providing an answer key that the algorithm can use to evaluate its
accuracy on training data.
\item An unsupervised model, in contrast, provides unlabeled data that the
algorithm tries to make sense of by extracting features and patterns on its own.
}

\subsection{Data representation}
\begin{transitionsubframe}
  \begin{center}
    \Huge Data representation
  \end{center}
\end{transitionsubframe}

\begin{frame}[fragile]{\textbf{Q. How do you represent data in ML?}}
  \begin{wideitemize}
    \item In general, the given data is expressed in a form of a bunch of vectors
    $\vec{v}_{j} \in {\Bbb R}^{d}$ that belong to some high dimensional vector space.
    \item For instance, in image recognition, the vector\footnote{a.k.a., feature
    vector; it could be numerical (e.g., height of tree) or descriptive
    (e.g., eye color)} of an each image is a set of pixels\footnote{each entry
    in the vector (e.g., pixel) represents a feature.} (i.e., a pixelated
    version of the image).
    \item If you have a notion of distance $\Delta(\vec{v}_{i}, \vec{v}_{j})$,
    then you can compare which vectors are close to each other in this high
    dimensional vector space; e.g., the norm $\norm{\vec{v}_{i} - \vec{v}_{j}}^2$.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile, label={distance}]{\textbf{Q. What is it important about measuring distances?}}
  \begin{wideitemize}
    \item Because the shorter the distance between two feature vectors the closer
    in character are the two samples they represent.
    \item There are many ways to measure distance:
    \begin{wideitemize}
      \item[-] Manhattan distance ($L^{2}$ norm).The sum of the absolute values of the different between
      entries in the vector. (Preferred distance when dealing with data in high dimensions.)
      \item[-] Euclidean distance ($L^{2}$ norm). Square the distances between vector entries,
      sum these and square root.
      \item[-] Cosine similarity. Cosine of the angle between two vectors\footnote{Two vectors
      might be similar if they are pointing in the same direction even if they are of different lengths.}.
      Just take the \textit{dot} product of two vectors and divide by the two lengths.
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile, label={CoD}]{\textbf{Q. Any other issues we should care to learn?}}
  \begin{wideitemize}
    \item The curse of dimensionality. This phenomena involves data in high dimensions.
    \item Suppose you are working in $M$ dimensions, that is you data has $M$
    features. And suppose you have $N$ data points. Having a large number of
    data points is good, the more the merrier. BUT what about number of features?
    \item Think how these $N$ data points might be distributed in $M$ dimensions.
    Suppose that the numerical data for each feature is $0$ or $1$. There will
    therefore be $2^{M}$ possible combinations.
    \item If $N$ is less than $2^{M}$ then you run the risk of having every data
    point being on a different corner of the $M$-dimensional hypercube.
  \end{wideitemize}
\end{frame}

\subsection{Process overview}
\begin{transitionsubframe}
  \begin{center}
    \Huge Building a Machine learning model
  \end{center}
\end{transitionsubframe}

\begin{frame}[fragile]{\textbf{Q. What does this process looks like?}}
  \begin{wideitemize}
    \item \textbf{Explore and clean the data}. Explore the data to really understand
    what those features look like, then we use some of these learnings to clean the data.
    \item \textbf{Split data into train/validation/test data sets}.
    \item \textbf{Fit an initial model and evaluate}. We will use 5-fold cross
    validation o the training set to fit an initial model to see what we can expect
    for the baseline performance of the model.
    \item \textbf{Tune hyper-parameters}. We use the same 5-fold cross validation
    paired with grid search to explore a variety of different hyper-parameter settings
    for each algorithm.
    \item \textbf{Evaluate on validation set}. We select the top model of each
    algorithm, and we evaluate them against each other on the validation set.
    \item \textbf{Select and evaluate the final model on the test set}. We select
    the top model based on the validation set and we will evaluate it on the test
    set to get an unbiased view of the model performance on completely unseen data.
  \end{wideitemize}
\end{frame}

\subsection{Measuring success}
\begin{transitionsubframe}
  \begin{center}
    \Huge Measuring success
  \end{center}
\end{transitionsubframe}


\begin{frame}[fragile]{\textbf{Q. Why do we split the data?}}
  \begin{wideitemize}
    \item To make sure that a model is learning the underlying pattern and not
    just memorizing the examples. We don't know how well the model will generalize
    because we don't have any additional data to test this.
    \item We can solve that by splitting our dataset into three separate segments;
    training (60\%), validation (20\%), and testing (20\%) data sets.
    \begin{wideitemize}
      \item training data, or examples, that the model will learn
      those general patterns from.
      \item validation data is used to select the best model (optimal algorithm
      and hyper-parameter settings).
      \item test data is used to provide an unbiased evaluation of what
      the model will look like in its real environment.
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What are the risks of not splitting the data?}}
  \begin{wideitemize}
    \item Overfitting or underfitting to the data
    \item Inaccurate representation of how the model will generalize
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is cross-validation?}}
  \begin{wideitemize}
    \item Cross-validation is a technique for assessing how well a model
    performs on new independent data (holdout test set\footnote{sample of the
    data not used in fitting a model; used to evaluate the model's ability to
    generalize to unseen examples}). (how robust the model is.)
    \item The simplest example of cross-validation is when you split your
    data into three groups\footnote{e.g., a \~{}60\%-\~{}20\%-\~{}20\% split}:
    (1) training data, (2) validation data, and (3) testing data.
    \item both validation and test data sets are a type of holdout set.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What do you do in (k-fold) cross-validation?}}
  \begin{wideitemize}
    \item Data is divided into $k$ subsets and the holdout method is repeated
    $k$ times. Each time, one of the $k$ subsets is used as the test set and the
    other $k-1$ subsets are combined to be used to train the model.
    \item E.g., given $10,000$ examples, we divide them into $k$ or $5$ subsets
    of data; each subset has $2000$ examples\footnote{this is sampling without replacement;
    no single example will appear in two different subsets and all original $10,000$
    are still accounted for in these subsets}.
    \item No we assign one of these $5$ subsets as a test set. Then, we fit a model
    on the training set of $8000$ examples and then we'll evaluate the model on
    the $2000$ example test set (holdout set). The performance metric is then
    recorded for this first iteration on the holdout set.
    \item On the second iteration, where select another holdout test, and we perform
    same steps as before.
  \end{wideitemize}
\end{frame}


\begin{frame}[fragile]{\textbf{Q. What are the components of the Evaluation framework?}}
  \begin{wideitemize}
    \item \textbf{Evaluation metrics}: how are gauging the accuracy of the model?
    \item \textbf{Process (how to split the data)}: how do we leverage a given
    data set to mitigate the likelihood of overfitting and underfitting.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What are the evaluation metrics that we'll use?}}
\begin{wideitemize}
  \item {\large \textbf{There isn't a one-size-fits-all metric.}}
  \item The metric(s) chosen to evaluate a ML model depends on various factors:
  \begin{wideitemize}
    \item Is it a regression or classification task? E.g., MSE vs Accuracy.
    \item What is the business objective? E.g., precision vs recall.
    \item What is the distribution of the target variable?
  \end{wideitemize}
  \item Examples of other metrics: Mean Absolute Error (MAE), Mean Squared Error
    (MSE), R-squared, Confusion Matrix and related metrics (Precision, Recall, Accuracy).
\end{wideitemize}
\end{frame}

\note[itemize]{
\item Regression: Mean Squared Percentage Error (MSPE), Mean Absolute Error (MAE),
Minimum Sum of Absolute Error (MSAE), Mean Squared Error (MSE), R-squared, Adjusted R-squared
\item Classification: Confusion Matrix and related metrics (Precision, Recall, Accuracy),
Receiver Operating Characteristics \& Area under the curve (ROC-AUC), log-loss, F1-score
}

\begin{frame}[fragile]{\textbf{Q. Metrics for classification problem?}}
\begin{wideitemize}
  \item So, for a classification problem, we're going to use three commonly
  used evaluation metrics. The first is accuracy:
  $\frac{\# predicted correctly}{total \# of examples}$
  \item The next evaluation metric is something called precision:
  $\frac{\# predicted as class A that actually class A}{total \# predicted to be in class A}$
  \item The last evaluation metric is something called recall:
  $\frac{\# predicted as class A that actually class A}{total \# that we actually in class A}$
\end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you explain what precision and recall are?}}
\begin{wideitemize}
  \item \textbf{Recall} is a measure of completeness or quantity, whereas
   \item \textbf{Precision} is a measure of exactness or quality:\vspace{.4em}
  \begin{itemize}
    \item \parbox[t]{1.5in}{$\underbrace{Recall = \frac{TP}{TP +
            FN}}_{\substack{\text{What proportion of actual positives}\\ \text{was identified correctly?}}}$} \hspace{.6in}
      \parbox[t]{1.5in}{$\underbrace{Precision = \frac{TP}{TP + FP}}_{\substack{\text{What
            proportion of positive identifications}\\\text{was actually correct?}}}$}
  \end{itemize}
  \item In simple terms, \textbf{high} precision means your algorithm has returned
  substantially \textit{more relevant results than irrelevant ones}, while
  \textbf{high} recall means your algorithm has returned \textit{most of the
  relevant results}.
\end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is a confusion matrix?}}
  \begin{wideitemize}
  \item A \textbf{Confusion Matrix} is a simple way of understanding \textbf{how well an
    algorithm is doing at classifying data}. It is just the idea of \textbf{false
      positives} and \textbf{false negatives}
  \item
    \begin{tabular}{cc|cc}
      \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted} \\
      \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No} \\ \hline
      \multirow[c]{2}{*}{\rotatebox[origin=tr]{90}{Actual}} & Yes & TP & FN \\[1.5ex] & No & FP & TN \\ \hline
    \end{tabular}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you explain what are false positives and false negatives?}}
  \begin{wideitemize}
  \item There are two errors that often rear their head when you are learning
    about hypothesis testing —- false positives and false negatives, technically
    referred to as type I error and type II error respectively.
  \item \textbf{False positives} are incorrect classifications of the presence
    of a condition when it is actually absent. A \textbf{false positive} is \textcolor{red}{when you
      reject a true null hypothesis}.
  \item \textbf{False negatives} are incorrect classifications of the absence of
    a condition when it is actually present. A \textbf{false negative} is \textcolor{red}{when you accept
      a false null hypothesis}.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Provide examples when false positives are more
      important than false negatives, false negatives are more important than
      false positives and when these two types of errors are equally important}}
  \begin{wideitemize}
  \item E.g., cancer screening. It is much worse to say that someone
    doesn't have cancer when they do (false negative) rather than saying that
    someone has it when in fact they don't (false positive).
  \item E.g., (from a psychological perspective) pregnancy test. Given the null
    hypothesis: ``I am not pregnant.'' A false negative for someone who really
    does not want a child, is not ready for one and when assuring themselves
    with a negative result.
  \item E.g., forecasting (health weather). During the covid-19 outbreak.
    Tracking rate of false positives and false negatives in March 2020 in the
    United States of America.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you list some of metrics derived from Confusion Matrix?}}
  \begin{wideitemize}
  \item Confusion matrix related metrics
    \begin{wideitemize}
    \item Accuracy rate: {\footnotesize $\frac{TP + TN}{Total}$}
      where {\footnotesize Total is $TP + TN + FP + FN$},\vspace{-.5em}
    \item Error rate: {\footnotesize $1 - \frac{TP + TN}{Total}$},\vspace{-.5em}
    \item False positive rate: {\footnotesize $\frac{FP}{TN + FP}$},\vspace{-.5em}
    \item Recall: {\footnotesize $\frac{TP}{TP + TN}$}, Precision: {\footnotesize $\frac{TP}{TP + FP}$},\vspace{-.5em}
    \item Specificity: {\footnotesize $1 - \frac{FP}{TN + FP}$},\vspace{-.5em}
    \item Prevalence: {\footnotesize $\frac{TP + FN}{Total}$}
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How do you measure how  good your
      classification algorithm is? You have unbalanced numbers, with our class
      being much larger or smaller than others}}
  \begin{wideitemize}
  \item You use the Matthews correlation coefficient. The number it yields is
    between plus or minus one. Plus one means perfect prediction, zero means no
    better than random.
  \item $\frac{TP x TN - FP x FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$
  \end{wideitemize}
\end{frame}


\begin{frame}[fragile]{\textbf{Q. Is there another way to measure how well an
      algorithm is classifying data? Also given an example}}
  \begin{wideitemize}
  \item Another way of looking at how well an algorithm is classifying is the
    \textbf{receiver operating characteristic} or \textbf{ROC} curve.
    \item E.g., suppose there is a threshold (or parameter) in your
      classification algorithm that determines whether you have an apple or a
      non apple object.
      \begin{wideitemize}
      \item[-] Plot the \textit{true positive} rate against the
        \textit{false positive} rate as the threshold (or parameter) varies.
        Points above the $45\degree$ diagonal are good, and the further
        into the top left of the plot the better.
      \item[-] The area under the ROC curve (the \textbf{AUC}\footnote{AUC is
          used to rank kaggle competitions}) is then a measure of how
        good different algorithms are. The closer to one (the max possible) the better.
      \end{wideitemize}
  \end{wideitemize}
\end{frame}

\section{Optimizing a Model}
\begin{transitionframe}
  \begin{center}
    \Huge Model optimization
  \end{center}
\end{transitionframe}

\begin{frame}[fragile]{\textbf{Q. Bias and Variance in Machine learning?}}
  \begin{wideitemize}
    \item Bias\footnote{High bias is the result of the algorithm missing the
    relevant relations between features and target outputs} is the algorithm's
    tendency to consistently learn the wrong thing by not taking into account
    all the information in the data. (results in inaccurate predictions).
    \item Variance\footnote{High variance is a result of the algorithm fitting to
    random noise in the training data} refers to an algorithm's sensitivity
    to small fluctuations in the training data set.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you be more specific about Bias and Variance?}}
  \begin{wideitemize}
    \item \textbf{Bias} is how far away (or error) the trained model is from the correct
    result \textit{on average}. Where ``on average'' means over many goes at
    training the model, using different data:
    \begin{wideitemize}
    \item $Bias(\hat{f}(x^{'})) = \underbrace{\mathbb{E}[\hat{f}(x)]}_{\text{Average error}} - \quad f(x^{'})$
    \end{wideitemize}
    \item \textbf{Variance} is a measure of the magnitude of that error.
    \begin{wideitemize}
    \item $Var(\hat{f}(x^{'})) = \mathbb{E}[\hat{f}(x)^{2}] - \mathbb{E}[\hat{f}(x^{'})]^{2}$
    \end{wideitemize}
    \item We often find there is a trade-off between bias and variance. As one is reduced,
    the other is increased\footnote{This the matter of over-and-underfitting}.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Bias and Variance tradeoff?}}
  \begin{wideitemize}
    \item Model complexity is across the x axis and model error across
    the y axis. More complexity means higher variance. Lesser complexity
    means higher bias.
    \item Total error is very high for very simple models and a very complex
    model, and then it bottoms out in the middle. So this what bias/variance
    tradeoff is all about: \textbf{finding the right model complexity} that
    minimizes both bias and variance (I mean the total error\footnote{Total
    error = (Bias + Variance) + Irreducible Error}) as much as possible.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Underfitting?}}
  \begin{wideitemize}
    \item (Recall high bias.) Underfitting occurs when an algorithm
    cannot capture the underlying trend of the data. This happens
    when the model is too simple with high bias and low variance, and
    results in high total error.
    \item Underfitting: High bias + low variance
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Overfitting?}}
  \begin{wideitemize}
    \item (Recall high variance.) Overfitting occurs when an algorithm
    fits too closely to a limited set of data (i.e., training set). In other
    words, the model might just memorize the examples that it has seen in
    the training data.
    \begin{wideitemize}
      \item Then when it sees a new example that looks a lot like an
      example it has seen before it can make a very accurate prediction.
      \item And when it sees a new example that looks nothing like any
      other examples it has seen before it will make a very poor prediction.
    \end{wideitemize}
    \item Overfitting: Low bias + high variance
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How do you find the optimal tradeoff?}}
  \begin{wideitemize}
    \item The goal is to find something in the middle (a model with medium
    complexity). This would learn the true pattern in the data w/o memorizing
    every example in the training data.
    \item Optimal tradeoff: Low bias + Low variance. OKAY, \textcolor{red}{but
    how do you identify underfit and overfit?}
    \begin{wideitemize}
      \item With underfit, we'll have high training error and high test error.
      \item Optimal tradeoff, we'll have low training error and low test error.
      \item With overfit, we'll have low training error and high test error.
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How do you tune a model for optimal complexity?}}
  \begin{wideitemize}
    \item There are two methods to tune a model for optimal complexity:
    \begin{wideitemize}
      \item Hyper-parameter tuning -- choosing a set of optimal hyper-parameters
      for fitting a ML algorithm (e.g., linear regression)
      \item Regularization -- a technique used specifically to reduce overfitting
      by discouraging overly complex models in some way.
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is a hyper-parameter?}}
  \begin{wideitemize}
    \item A model \textbf{parameter} is a configuration variable that
    is internal to the model and \textit{whose value can be estimated from data}.
    \item A model \textbf{hyper-parameter} is a configuration that is external
    to the model and \textit{whose value cannot estimated from data}, and
    \textit{whose value guides how the algorithm learns parameter values from
    the data}. E.g., depth of a decision tree is a model hyper-parameter vs
    ticket price or ticket class are model parameters.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Regularization?}}
  \begin{wideitemize}
    \item Regularization is a form of regression, which constrains/regularizes or shrinks
    the (learned) coefficient estimates towards zero. In other words, this technique
    reduces overfitting by discouraging learning a more complex model in some way.
  \end{wideitemize}
  \begin{framed}
    The goal of Regularization is to allow enough flexibility for the algorithm
    to learn the underlying patterns in the data but provides guardrails so it
    doesn't overfit. See Occam's razor -- whenever possible, choose the simplest
    answer (or model) to a problem.
  \end{framed}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you provide Regularization examples?}}
  \begin{wideitemize}
    \item \textbf{Ridge regression and lasso regression}: adding a penalty
    to the loss function (See Model fitting slide) to constrain coefficients.
    \item \textbf{Dropout}: some nodes are ignored during training which
    forces the other nodes to take on more or less responsibility for the
    input-output.
  \end{wideitemize}
\end{frame}

\subsection{Model fitting}
\begin{transitionsubframe}
  \begin{center}
    \Huge Specifics to Model fitting
  \end{center}
\end{transitionsubframe}

\begin{frame}[fragile]{\textbf{Q. What is Machine learning function fitting?}}
  \begin{wideitemize}
    \item Model fitting is a measure of how well a machine learning model
    generalizes to similar data to that on which it was trained.
    \item A model that is well-fitted produces more accurate outcomes. A model
    that is overfitted matches the data too closely. A model that is underfitted
    doesn't match closely enough.
  \end{wideitemize}
\end{frame}


\begin{frame}[fragile]{\textbf{Q. How do you measure a ML model performance for given data?}}
  \begin{wideitemize}
    \item We do that by using a cost function.
    \item In ML, a cost function (or loss function) is used to represent how far away a mathematical
    model is from the real data.
    \begin{itemize}
      \item A common way to do this is via the quadratic cost function\footnote{This is
      just the sum of the squares of the vertical distances between the points and the straight line}:\vspace{.4em}
      $J(\theta) = \frac{1}{2N}\sum^{N}_{n=1}(h_{\theta}(x^{(n)}) - y^{(n)})^{2}$.\vspace{.4em}
      \item We are interested in the parameters that minimize this quadratic cost function.
      This is called ordinary least squares (OLS).
    \end{itemize}
    \item One adjusts the mathematical model usually by varying parameters within the model, so as
    to \textit{minimize the cost function}. This is interpreted as given the best model, of
    its type, that fits the data.
  \end{wideitemize}
\end{frame}


\begin{frame}[fragile]{\textbf{Q. What is Gradient Descent?}}
  \begin{wideitemize}
    \item Gradient descent is an optimization algorithm used to minimize
    some \textit{convex} function by iteratively moving in the direction of
    steepest descent as defined by the negative of the gradient.
    \item In ML, we use gradient descent to update the parameters of a ML model.
    \begin{wideitemize}
      \item Start with an initial guess for each parameter $\theta_{k}$. Then
      move $\theta_{k}$ in the \\direction of the slope.
      \item Update all $\theta_{k}$ simultaneously (known as batch gradient descent),
      and \\repeat until convergence.
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Stochastic Gradient Descent?}}
  \begin{wideitemize}
    \item Similar to batch gradient descent except that you only update
    using one of the data points each time.
    \item And that data point is chosen randomly. Hence the stochastic.
    \item In other words, stochastic gradient descent pick an $n$ at random
    and then update using one of the data points. Repeat, picking another
    data point at random, etc.
  \end{wideitemize}
\end{frame}

\subsection{Training, Testing and Validation}
\begin{transitionsubframe}
  \begin{center}
    \Huge Training, Testing and Validation
  \end{center}
\end{transitionsubframe}


\begin{frame}[fragile]{\textbf{Q. What is training?}}
  \begin{wideitemize}
    \item Most ML algorithms need to be trained. That is, you give
    them data and they look for patterns, or best fits, etc.
    \item They know they are doing well when perhaps a loss function
    has been minimized, or the rewards have been maximized.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is an epoch?}}
  \begin{wideitemize}
    \item In some ML methods, one uses the same training data many times,
    as the algorithm gradually converges, for example, in stochastic gradient
    descent. Each time the whole training set of data is used in the training
    that is called an \textbf{epoch or iteration}\footnote{Typically you won't
    get decent results until convergence after many epochs}.
    \item One might see a decreasing error as the number of epochs increases.
    But that doesn't mean your algorithm is getting better, it could easily
    mean that you are overfitting\footnote{This could happen if the learning
    algorithm has seen the training data so many times or epochs}. To test for
    this you introduce a test data set, the data that you've held back.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Any caveats on training and testing?}}
  \begin{wideitemize}
    \item Over many epochs, if the test error begins to rise (and it's much
    bigger than the training error) then you have overfitted.
    \item To help avoid overfitting sometimes we divide up our original data
    into three sets. The third data set is the validation data set.
    (One can use cross validation to evaluate performance of model using
    training, test, and validation data sets.)
  \end{wideitemize}
\end{frame}

\section{Algorithms}
\begin{transitionframe}
  \begin{center}
    \Huge Machine Learning Algorithms
  \end{center}
\end{transitionframe}


\begin{frame}[fragile]{\textbf{Q. What is linear regression?}}
  \begin{wideitemize}
  \item Linear regression attempts to model the relationship between two continuous
    variables -- a scalar response (or dependent variable) and and one or more
    explanatory variables (or independent variables) -- by fitting a linear
    equation to observed data.
  \item The governing equation for linear regression is also quite simple. It
    takes the form: $y = Bx + A$ where $y$ is the dependent variable, $x$ is the
    independent variable, and $A$ and $B$ are coefficients dictating the equation
  \item e.g. a model that assumes a linear relationship between the input
    variables $x$ and the single output variable $y$.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is the general form of multiple regression?}}
  \begin{wideitemize}
    \item The general form of the equation for linear regression.
  \end{wideitemize}
\end{frame}


\end{document}
