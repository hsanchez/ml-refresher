\input{preamble}
%% end of preamble

\usepackage{framed}
\usepackage[inline]{enumitem}
\usepackage[backend=bibtex]{biblatex}

\title[]{\Huge \textbf{\textcolor{black}{A Machine Learning Refresher}}}
\subtitle{\Large Q \& A on the main concepts and terminology}
% \author{\large \textbf{Huascar A. Sanchez}}
\author[HAS]{
% \author[HAS \& DS]{
\parbox[t]{1.5in}{Huascar Sanchez \\\small\texttt{huascar.sanchez@sri.com}} %\hspace{.3in}
% \and
% \parbox[t]{1.5in}{Second Author \\\small\texttt{second.author@sri.com}}
}
\institute[]{\large SRI International}

\date{\today}

% Left align title page
\makeatletter
\setbeamertemplate{title page}[default][left,colsep=-4bp,rounded=true,shadow=\beamer@themerounded@shadow]
\makeatother

\begin{document}
%%% TIKZ STUFF
\tikzset{
        every picture/.style={remember picture,baseline},
        every node/.style={anchor=base,align=center,outer sep=1.5pt},
        every path/.style={thick},
        }
\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-a) at (-.3em,.3em) {};%
}
\newcommand\markbottomright[2]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-b) at (0em,0em) {};%
}
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{mybox} =[draw=black, very thick, rectangle, inner sep=10pt, inner ysep=20pt]
\tikzstyle{mybox2} =[draw=black, very thick, rectangle, inner sep=5pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[draw=black,fill=red, text=white]
%%%% END TIKZ STUFF

% Title Slide
\begin{frame}
\maketitle
% \centering
\tiny\hspace{1em}The views expressed do not necessarily reflect the position of my employer.
\end{frame}

% INTRO
\section{Warmup}
\begin{frame}[fragile]{\textbf{Q. What is cross-validation?}}
  \begin{wideitemize}
    \item Cross-validation is a technique for assessing how well a model
    performs on new independent data.
    \item The simplest example of cross-validation is when you split your
    data into two groups\footnote{e.g., a \~{}60\%-\~{}40\% split}:
    (1) training data, and (2) testing data.
    \item One uses training data to build the model and testing
    data to test the model.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How to define/select metrics?}}
\begin{wideitemize}
  \item {\large \textbf{There isn't a one-size-fits-all metric.}}
  \item The metric(s) chosen to evaluate a ML model depends on various factors:
  \begin{wideitemize}
    \item Is it a regression or classification task?
    \item What is the business objective? E.g., precision vs recall.
    \item What is the distribution of the target variable?
  \end{wideitemize}
  \item E.g., Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared,
  Confusion Matrix and related metrics (Precision, Recall, Accuracy).
\end{wideitemize}
\end{frame}

\note[itemize]{
\item Regression: Mean Squared Percentage Error (MSPE), Mean Absolute Error (MAE),
Minimum Sum of Absolute Error (MSAE), Mean Squared Error (MSE), R-squared, Adjusted R-squared
\item Classification: Confusion Matrix and related metrics (Precision, Recall, Accuracy),
Receiver Operating Characteristics \& Area under the curve (ROC-AUC), log-loss, F1-score
}


\begin{frame}[fragile]{\textbf{Q. Can you explain what precision and recall are?}}
\begin{wideitemize}
  \item \textbf{Recall} is a measure of completeness or quantity, whereas
  \textbf{precision} is a measure of exactness or quality:\vspace{.2em}
  \begin{itemize}
    \item \parbox[t]{1.5in}{$Recall = \frac{TP}{TP + FN}$} \hspace{.3in}
      \parbox[t]{1.5in}{$Precision = \frac{TP}{TP + FP}$}
  \end{itemize}
  \item In simple terms, \textbf{high} precision means your algorithm has returned
  substantially \textit{more relevant results than irrelevant ones}, while
  \textbf{high} recall means your algorithm has returned \textit{most of the
  relevant results}.
\end{wideitemize}
\end{frame}

\note[itemize]{
\item \textbf{Recall} answers ``What proportion of \textit{actual}
positives was identified correctly?''
\begin{equation*}
  Recall = \frac{TP}{TP + FN}
\end{equation*}
\item \textbf{Precision} answers ``What proportion of positive
identifications was actually correct''
\begin{equation*}
  Precision = \frac{TP}{TP + FP}
\end{equation*}
}


\begin{frame}[fragile]{\textbf{Q. Can you explain what are false positives and
false negatives?}}
\begin{wideitemize}
  \item \textbf{False positives} are incorrect classifications of the presence
  of a condition when it is actually absent.
  \item \textbf{False negatives} are incorrect classifications of the absence
  of a condition when it is actually present.
\end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Provide examples when false positives are more
important than false negatives, false negatives are more important than false
positives and when these two types of errors are equally important}}
\begin{wideitemize}
  \item TODO
\end{wideitemize}
\end{frame}

\end{document}
