\input{preamble}
%% end of preamble

\usepackage{framed}
\usepackage[inline]{enumitem}
\usepackage[backend=bibtex]{biblatex}
\usepackage{amssymb}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\title[]{\Huge \textbf{\textcolor{black}{A Machine Learning Refresher}}}
\subtitle{\Large Q \& A on the main concepts and terminology}
% \author{\large \textbf{Huascar A. Sanchez}}
\author[HAS]{
% \author[HAS \& DS]{
\parbox[t]{1.5in}{Huascar Sanchez \\\small\texttt{huascar.sanchez@sri.com}} %\hspace{.3in}
% \and
% \parbox[t]{1.5in}{Second Author \\\small\texttt{second.author@sri.com}}
}
\institute[]{\large SRI International}

\date{\today}

% Left align title page
\makeatletter
\setbeamertemplate{title page}[default][left,colsep=-4bp,rounded=true,shadow=\beamer@themerounded@shadow]
\makeatother

\begin{document}
%%% TIKZ STUFF
\tikzset{
        every picture/.style={remember picture,baseline},
        every node/.style={anchor=base,align=center,outer sep=1.5pt},
        every path/.style={thick},
        }
\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-a) at (-.3em,.3em) {};%
}
\newcommand\markbottomright[2]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-b) at (0em,0em) {};%
}
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{mybox} =[draw=black, very thick, rectangle, inner sep=10pt, inner ysep=20pt]
\tikzstyle{mybox2} =[draw=black, very thick, rectangle, inner sep=5pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[draw=black,fill=red, text=white]
%%%% END TIKZ STUFF

% Title Slide
\begin{frame}
\maketitle
% \centering
\tiny\hspace{1em}The views expressed do not necessarily reflect the position of my employer.
\end{frame}

% INTRO
\section{Warmup}

\begin{frame}[fragile]{\textbf{Q. What is Machine Learning?}}
  \begin{wideitemize}
    \item Machine Learning is fitting a function to examples and using
    that function to generalize and make predictions about
    new examples.
    \item Machine Learning, by large, falls into two categories:\vspace{.4em}
    \begin{wideitemize}
      \item[-] Supervised learning
      \item[-] Unsupervised learning
    \end{wideitemize}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How do you represent data in ML?}}
  \begin{wideitemize}
    \item In general, the given data is expressed in a form of a bunch of vectors
    $\vec{v}_{j} \in {\Bbb R}^{d}$ that belong to some high dimensional vector space.
    \item For instance, in image recognition, the vector of an each image is
    a set of pixels (i.e., a pixelated version of the image).
    \item If you have a notion of distance $\Delta(\vec{v}_{i}, \vec{v}_{j})$,
    then you can compare which vectors are close to each other in this high
    dimensional vector space; e.g., the norm $\norm{\vec{v}_{i} - \vec{v}_{j}}^2$
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Supervised Learning?}}
  \begin{wideitemize}
    \item In \textbf{Supervised Learning} (or \textbf{SL}), you are given a
    bunch of examples and their labels (e.g., A or B) and the goal is to
    classify (or assign), when you are given a new example, to which label we
    assign the new example.
    \item \textcolor{blue}{You could think of these labels the name of the
    \textbf{class or cluster} to which certain portions of the data belong.}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you give an example of Supervised Learning?}}
  \vspace{.4em}
  \begin{wideitemize}
    \item \textcolor{blue}{\textbf{Support vector machines} or \textbf{SVM}}
    \item In \textbf{SVM}, the goal is to construct the optimal separating
    \textit{hyperplane} between pieces of data; e.g., between clusters of data
    represented by labels A and B.\vspace{.4em}
    \begin{wideitemize}
      \item[-] These clusters sit in some high dimensional space, and the idea is to
      construct a plane that maximizes the margins between the plane and the data.
      \item[-] If a new datum sits closer to one area of the data, say A, then
      we assign this new datum to A.
    \end{wideitemize}
    \item (For historical reasons) This algorithm is called \textbf{support
    vector machines} because the vectors that lie on the margin of the plan are
    called the \textit{support} vectors.
  \end{wideitemize}

  \begin{framed}
  This is a method for constructing a device to discriminate. If we're having
  a supervised learning problem then this method gives me an optimal form of
  discrimination.
  \end{framed}

  \note[itemize]{
  \item This algorithm has an exponential speed up when you do it
  quantum mechanically..
  }
\end{frame}

\begin{frame}[fragile]{\textbf{Q. What is Unsupervised Learning?}}
  \begin{wideitemize}
    \item In \textbf{Unsupervised Learning} (or \textbf{UL}), you are given a
    bunch of data and you are not told it falls naturally into clusters, but
    you are not told what the clusters are.
    \item \textcolor{blue}{The goal is identify the clusters of data, how many clusters there are,
    and then be able to assign new things to these different clusters.}
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Can you give an example of Unsupervised Learning?}}
  \begin{wideitemize}
    \item \textcolor{blue}{Principal Component Analysis (or \textbf{PCA})} is a classical UL algorithm.
    \item In \textbf{PCA}, the way this works, we construct a \textit{covariance matrix},
    and my covariance matrix is just the following object: $C = \sum_{j} \vec{v}_j\vec{v}^{\dagger}_{j}$,
    where $\vec{v}^{T}_{j}$ is the transpose of $\vec{v}_{j}$.
    \begin{wideitemize}
      \item[-] In other words, we construct $C$ from the data by taking these vectors $\vec{v}_j$ and
      multiply them by their transpose $\vec{v}^{\dagger}_j$.
      \item E.g., Financial forecasting: the vectors could be, for example, can
      represent the changes in stock prices over a 24 hr period, and the
      covariance matrix $C$ would give the correlations (or covariances in the
      data) between the prices of the different stocks in different times within
      the 24 hrs period.
    \end{wideitemize}
    \item In \textbf{PCA}, you diagonalize $C$ and say $C = \sum_{k} P_k
    \vec{\omega}_{k}\vec{\omega}^{\dagger}_{k}$, $P_k$ is piece of the data with size $k$,
    and $\vec{\omega}_{k}$ are the set of vectors you need to find.
    \item If and only if a small set of $P_k >> 0$, then $C$ is effectively low-rank,
    and the corresponding $\vec{\omega}_{k}$ are the principal components.
  \end{wideitemize}
  \note[itemize]{
  \item In this stocks example, for instance, if it turns out that all the
  motions of stocks in the stocks market are highly correlated with each other AND
  there is only a few forms of correlations, then this matrix $C$ will be
  effectively low-rank and will only have a few principle components.
  \item You see, this PCA is a way of compressing the data. That is, if there
  is only a few principal components then this means that all my vectors can
  be written as the sum of a few $w$s.
  \item In general terms, this PCA process is about finding the underlying
  patterns of the data and also gives you a method for data compression.
  \item PCA is all about diagonalizing the covariance matrix.
  \item PCA is an exercise in linear algebra on very high dimensional vector spaces.
  }
\end{frame}

\section{Specifics}
\begin{transitionframe}
  \begin{center}
    \Huge Harder questions, please!
  \end{center}
\end{transitionframe}



\begin{frame}[fragile]{\textbf{Q. What is cross-validation?}}
  \begin{wideitemize}
    \item Cross-validation is a technique for assessing how well a model
    performs on new independent data.
    \item The simplest example of cross-validation is when you split your
    data into two groups\footnote{e.g., a \~{}60\%-\~{}40\% split}:
    (1) training data, and (2) testing data.
    \item One uses training data to build the model and testing
    data to test the model.
  \end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. How to define/select metrics?}}
\begin{wideitemize}
  \item {\large \textbf{There isn't a one-size-fits-all metric.}}
  \item The metric(s) chosen to evaluate a ML model depends on various factors:
  \begin{wideitemize}
    \item Is it a regression or classification task?
    \item What is the business objective? E.g., precision vs recall.
    \item What is the distribution of the target variable?
  \end{wideitemize}
  \item E.g., Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared,
  Confusion Matrix and related metrics (Precision, Recall, Accuracy).
\end{wideitemize}
\end{frame}

\note[itemize]{
\item Regression: Mean Squared Percentage Error (MSPE), Mean Absolute Error (MAE),
Minimum Sum of Absolute Error (MSAE), Mean Squared Error (MSE), R-squared, Adjusted R-squared
\item Classification: Confusion Matrix and related metrics (Precision, Recall, Accuracy),
Receiver Operating Characteristics \& Area under the curve (ROC-AUC), log-loss, F1-score
}


\begin{frame}[fragile]{\textbf{Q. Can you explain what precision and recall are?}}
\begin{wideitemize}
  \item \textbf{Recall} is a measure of completeness or quantity, whereas
  \textbf{precision} is a measure of exactness or quality:\vspace{.2em}
  \begin{itemize}
    \item \parbox[t]{1.5in}{$Recall = \frac{TP}{TP + FN}$} \hspace{.3in}
      \parbox[t]{1.5in}{$Precision = \frac{TP}{TP + FP}$}
  \end{itemize}
  \item In simple terms, \textbf{high} precision means your algorithm has returned
  substantially \textit{more relevant results than irrelevant ones}, while
  \textbf{high} recall means your algorithm has returned \textit{most of the
  relevant results}.
\end{wideitemize}
\end{frame}

\note[itemize]{
\item \textbf{Recall} answers ``What proportion of \textit{actual}
positives was identified correctly?''
\begin{equation*}
  Recall = \frac{TP}{TP + FN}
\end{equation*}
\item \textbf{Precision} answers ``What proportion of positive
identifications was actually correct''
\begin{equation*}
  Precision = \frac{TP}{TP + FP}
\end{equation*}
}


\begin{frame}[fragile]{\textbf{Q. Can you explain what are false positives and
false negatives?}}
\begin{wideitemize}
  \item \textbf{False positives} are incorrect classifications of the presence
  of a condition when it is actually absent.
  \item \textbf{False negatives} are incorrect classifications of the absence
  of a condition when it is actually present.
\end{wideitemize}
\end{frame}

\begin{frame}[fragile]{\textbf{Q. Provide examples when false positives are more
important than false negatives, false negatives are more important than false
positives and when these two types of errors are equally important}}
\begin{wideitemize}
  \item TODO
\end{wideitemize}
\end{frame}

\end{document}
